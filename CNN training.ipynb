{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN training.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"yZHFaDgdHNE9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Necessary imports\n","from __future__ import division, print_function, absolute_import\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6Sll4JPTH8EO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# read data from google drive\n","! pip install pydrive\n","# these classes allow you to request the Google drive API\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive \n","from google.colab import auth \n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","file_id = '12k_kl_Jc5O7_AT2dS87s3mQJtVpLEaRn'\n","downloaded = drive.CreateFile({'id': file_id})\n","# allows you to temporarily load your file in the notebook VM\n","\n","# assume the file is called file.csv and it's located at the root of your drive\n","downloaded.GetContentFile('aug_data.npy')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lM9GJAA_IFdq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# import augmented data\n","data = np.load(\"aug_data.npy\")\n","np.random.shuffle(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PthR9V9ZLWgp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["targets = data[:, 1]\n","data = np.stack(data[:, 0], axis=0)\n","split = np.array_split(data, 10, axis=0)\n","train_data = np.concatenate(split[0:7], axis=0)\n","test_data = np.concatenate(split[7:10], axis=0)\n","\n","# converting labels to one-hot vector\n","b = np.zeros((8020, 2))\n","b[np.arange(8020), list(targets)] = 1\n","train_targets = np.concatenate(np.array_split(b, 10, axis=0)[0:7], axis=0)\n","test_targets = np.concatenate(np.array_split(b, 10, axis=0)[7:10], axis=0)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HVGaDciJLsga","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","# Training Parameters\n","learning_rate = 0.001\n","# batch_size = 10\n","\n","# Network Parameters\n","num_input = 5625 * 3  # flattened data input (input shape: 75*75*3)\n","num_classes = 2  # number of classes (is ship or iceberg)\n","dropout = 0.75  # Dropout, probability to keep units\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oBOxH-FrMDBi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","class CNN:\n","    def __init__(self):\n","        \"\"\"\n","        In this class we will be performing 2-D convolution and training of the model.\n","        for now we are using 3 convolution layers and each layer is followed by a\n","        pooling operation.\n","        for each convolution layer, we are using a 5x5 filter with stride 1 on every dimension.\n","        ReLU activation is used after each convolution layer operation.\n","        after convolution layers, a fully connected layer of 1024 units is used to compute logits\n","        \"\"\"\n","        self.X = tf.placeholder(tf.float32, [None, num_input])  # flattened input\n","        self.Y = tf.placeholder(tf.float32, [None, num_classes])  # one-hot vector of labels\n","        self.keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n","\n","        self.weights = {\n","            # 3x3 conv, 1 input, 32 outputs\n","            'wc1': tf.Variable(tf.random_normal([3, 3, 3, 64])),\n","            # 5x5 conv, 32 inputs, 64 outputs\n","            'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n","            # 5x5 conv, 64 inputs, 128 outputs\n","            'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n","            # fully connected, 8*8*128 inputs, 512 outputs\n","            'wd1': tf.Variable(tf.random_normal([10 * 10 * 256, 512])),\n","            # 512 inputs, 2 outputs (class prediction)\n","            'out': tf.Variable(tf.random_normal([512, num_classes]))\n","        }\n","\n","        self.biases = {\n","            'bc1': tf.Variable(tf.random_normal([64])),\n","            'bc2': tf.Variable(tf.random_normal([128])),\n","            'bc3': tf.Variable(tf.random_normal([256])),\n","            'bd1': tf.Variable(tf.random_normal([512])),\n","            'out': tf.Variable(tf.random_normal([num_classes]))\n","        }\n","\n","        # Construct compute graph\n","        logits = self.conv_net(self.X, self.weights, self.biases, self.keep_prob)\n","        self.prediction = tf.nn.softmax(logits)\n","\n","        # Define loss and optimizer\n","        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","            logits=logits, labels=self.Y))\n","        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","        self.train_op = optimizer.minimize(self.loss_op)\n","\n","        # model evaluation\n","        correct_pred = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.Y, 1))\n","        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","        self.session = tf.Session()\n","        self.session.run(tf.global_variables_initializer())\n","\n","    def conv2d(self, x, W, b, strides=1):\n","        # Conv2D wrapper, with bias and ReLU activation\n","        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n","        x = tf.nn.bias_add(x, b)\n","        return tf.nn.relu(x)\n","\n","    def maxpool2d(self, x, ksize=2, k=2):\n","        # MaxPool2D wrapper\n","        return tf.nn.max_pool(x, ksize=[1, ksize, ksize, 1], strides=[1, k, k, 1],\n","                              padding='SAME')\n","\n","    # model creation\n","    def conv_net(self, x, weights, biases, dropout):\n","        # data input is a 1-D vector of 5625 features (75*75 data points)\n","        # Reshape to match picture format [Height x Width x Channel]\n","        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n","        x = tf.reshape(x, shape=[-1, 75, 75, 3])\n","\n","        # Convolution Layer 1\n","        conv1 = self.conv2d(x, weights['wc1'], biases['bc1'])\n","        # Max Pooling\n","        conv1 = self.maxpool2d(conv1, k=2)\n","\n","        # Convolution Layer 2\n","        conv2 = self.conv2d(conv1, weights['wc2'], biases['bc2'])\n","        # Max Pooling\n","        conv2 = self.maxpool2d(conv2, k=2)\n","\n","        # Convolution Layer 3\n","        conv3 = self.conv2d(conv2, weights['wc3'], biases['bc3'])\n","        # Max Pooling\n","        conv3 = self.maxpool2d(conv3, k=2)\n","\n","        # Fully connected layer\n","        # Reshape conv2 output to fit fully connected layer input\n","        fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n","        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n","        fc1 = tf.nn.relu(fc1)\n","        # Apply Dropout\n","        fc1 = tf.nn.dropout(fc1, dropout)\n","\n","        # Output, class prediction\n","        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n","        return out\n","\n","    def test_accuracy(self, data, targets):\n","        return self.session.run(self.accuracy, feed_dict={self.X: data,\n","                                                          self.Y: targets,\n","                                                          self.keep_prob:1.0})\n","\n","    def train(self, train_data, train_targets, batch_size=1, epochs=1):\n","        sess = self.session\n","        saver = tf.train.Saver()\n","        print(\"training started ...\")\n","        batch = 0\n","        test_datum = [test_data.flatten().transpose() for test_data in test_data]\n","        for epoch in range(epochs):\n","            print(\"\\n\\nepoch : %d\"%epoch)\n","            for datum, target in zip([train_data[i:i + batch_size] for i in range(0, len(train_data), batch_size)],\n","                                     [train_targets[i:i + batch_size] for i in\n","                                      range(0, len(train_targets), batch_size)]):\n","                datum = np.array(datum)\n","                datum = [datum.flatten().transpose() for datum in datum]\n","                sess.run(self.train_op, feed_dict={self.X: datum,\n","                                                   self.Y: target,\n","                                                   self.keep_prob: dropout})\n","                batch += 1\n","                if batch % 10 == 0 or batch == 1:\n","                    # Calculate batch loss and accuracy\n","                    loss, acc = sess.run([self.loss_op, self.accuracy], feed_dict={self.X: datum,\n","                                                                                   self.Y: target,\n","                                                                                   self.keep_prob: 1.0})\n","                    log_loss = tf.losses.log_loss(labels=target, predictions=sess.run(self.prediction,\n","                                                                                      feed_dict={self.X: datum,\n","                                                                                                 self.Y: target,\n","                                                                                                 self.keep_prob:1.0} ))\n","                    print(\"Step \" + str(batch) + \", Minibatch log Loss= \" + \\\n","                          \"%f\"%(sess.run(log_loss)) + \", Training Accuracy= \" + \\\n","                          \"{:.3f}\".format(acc))\n","                    # print(\"\\t Test accuracy : {}\".format(self.test_accuracy(test_datum, test_targets)))\n","\n","        print(\"\\n\\noptimization complete!\")\n","        save_path = saver.save(sess, \"saved_model/model.ckpt\")\n","        print(\"Model saved in path: %s\" % save_path)\n","        print(\"\\t test accuracy : {}\".format(self.test_accuracy(test_datum, test_targets)))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kUwxMkNkMHK6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":6452},"outputId":"a04f4d56-f176-4315-a3c8-48407baec07b","executionInfo":{"status":"ok","timestamp":1533663591460,"user_tz":180,"elapsed":1656202,"user":{"displayName":"shakti singh","photoUrl":"//lh3.googleusercontent.com/-glB83mv1E78/AAAAAAAAAAI/AAAAAAAAABI/DzZHRL7ZArY/s50-c-k-no/photo.jpg","userId":"113300376016354214540"}}},"cell_type":"code","source":["c = CNN()\n","c.train(train_data, train_targets, batch_size=10, epochs=6)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training started ...\n","\n","\n","epoch : 0\n","Step 1, Minibatch log Loss= 12.894476, Training Accuracy= 0.200\n","Step 10, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 20, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 30, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 40, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 50, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 60, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 70, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 80, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 90, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 100, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 110, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 120, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 130, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 140, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 150, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 160, Minibatch log Loss= 14.506287, Training Accuracy= 0.100\n","Step 170, Minibatch log Loss= 9.670857, Training Accuracy= 0.400\n","Step 180, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 190, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 200, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 210, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 220, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 230, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 240, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 250, Minibatch log Loss= 9.670857, Training Accuracy= 0.400\n","Step 260, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 270, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 280, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 290, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 300, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 310, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 320, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 330, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 340, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 350, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 360, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 370, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 380, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 390, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 400, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 410, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 420, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 430, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 440, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 450, Minibatch log Loss= 9.670857, Training Accuracy= 0.400\n","Step 460, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 470, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 480, Minibatch log Loss= 9.670857, Training Accuracy= 0.400\n","Step 490, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 500, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 510, Minibatch log Loss= 9.670857, Training Accuracy= 0.400\n","Step 520, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 530, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 540, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 550, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 560, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","\n","\n","epoch : 1\n","Step 570, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 580, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 590, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 600, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 610, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 620, Minibatch log Loss= 9.670857, Training Accuracy= 0.400\n","Step 630, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 640, Minibatch log Loss= 12.894476, Training Accuracy= 0.200\n","Step 650, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 660, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 670, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 680, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 690, Minibatch log Loss= 11.282667, Training Accuracy= 0.300\n","Step 700, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 710, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 720, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 730, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 740, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 750, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 760, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 770, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 780, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 790, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 800, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 810, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 820, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 830, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 840, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 850, Minibatch log Loss= 8.059048, Training Accuracy= 0.500\n","Step 860, Minibatch log Loss= 8.727926, Training Accuracy= 0.400\n","Step 870, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 880, Minibatch log Loss= 4.845942, Training Accuracy= 0.700\n","Step 890, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 900, Minibatch log Loss= 5.760748, Training Accuracy= 0.600\n","Step 910, Minibatch log Loss= 8.438322, Training Accuracy= 0.300\n","Step 920, Minibatch log Loss= 4.027506, Training Accuracy= 0.600\n","Step 930, Minibatch log Loss= 9.651413, Training Accuracy= 0.400\n","Step 940, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 950, Minibatch log Loss= 6.476021, Training Accuracy= 0.600\n","Step 960, Minibatch log Loss= 9.670852, Training Accuracy= 0.400\n","Step 970, Minibatch log Loss= 5.396529, Training Accuracy= 0.600\n","Step 980, Minibatch log Loss= 1.611809, Training Accuracy= 0.900\n","Step 990, Minibatch log Loss= 4.375443, Training Accuracy= 0.600\n","Step 1000, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 1010, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 1020, Minibatch log Loss= 3.223619, Training Accuracy= 0.800\n","Step 1030, Minibatch log Loss= 3.223624, Training Accuracy= 0.800\n","Step 1040, Minibatch log Loss= 4.832062, Training Accuracy= 0.700\n","Step 1050, Minibatch log Loss= 3.192433, Training Accuracy= 0.700\n","Step 1060, Minibatch log Loss= 2.788554, Training Accuracy= 0.800\n","Step 1070, Minibatch log Loss= 0.004551, Training Accuracy= 1.000\n","Step 1080, Minibatch log Loss= 4.835429, Training Accuracy= 0.700\n","Step 1090, Minibatch log Loss= 6.476257, Training Accuracy= 0.600\n","Step 1100, Minibatch log Loss= 3.391370, Training Accuracy= 0.700\n","Step 1110, Minibatch log Loss= 3.218848, Training Accuracy= 0.800\n","Step 1120, Minibatch log Loss= 4.649974, Training Accuracy= 0.600\n","\n","\n","epoch : 2\n","Step 1130, Minibatch log Loss= 3.634454, Training Accuracy= 0.700\n","Step 1140, Minibatch log Loss= 2.525390, Training Accuracy= 0.800\n","Step 1150, Minibatch log Loss= 4.981277, Training Accuracy= 0.600\n","Step 1160, Minibatch log Loss= 6.460580, Training Accuracy= 0.600\n","Step 1170, Minibatch log Loss= 6.821922, Training Accuracy= 0.500\n","Step 1180, Minibatch log Loss= 9.807172, Training Accuracy= 0.300\n","Step 1190, Minibatch log Loss= 3.347437, Training Accuracy= 0.700\n","Step 1200, Minibatch log Loss= 8.194370, Training Accuracy= 0.400\n","Step 1210, Minibatch log Loss= 9.399135, Training Accuracy= 0.400\n","Step 1220, Minibatch log Loss= 4.711043, Training Accuracy= 0.700\n","Step 1230, Minibatch log Loss= 4.999366, Training Accuracy= 0.600\n","Step 1240, Minibatch log Loss= 3.851552, Training Accuracy= 0.700\n","Step 1250, Minibatch log Loss= 7.996438, Training Accuracy= 0.400\n","Step 1260, Minibatch log Loss= 4.123311, Training Accuracy= 0.700\n","Step 1270, Minibatch log Loss= 2.149662, Training Accuracy= 0.600\n","Step 1280, Minibatch log Loss= 4.865910, Training Accuracy= 0.700\n","Step 1290, Minibatch log Loss= 3.628831, Training Accuracy= 0.500\n","Step 1300, Minibatch log Loss= 4.881639, Training Accuracy= 0.700\n","Step 1310, Minibatch log Loss= 3.518977, Training Accuracy= 0.600\n","Step 1320, Minibatch log Loss= 1.661736, Training Accuracy= 0.900\n","Step 1330, Minibatch log Loss= 3.637366, Training Accuracy= 0.700\n","Step 1340, Minibatch log Loss= 1.909955, Training Accuracy= 0.500\n","Step 1350, Minibatch log Loss= 6.039067, Training Accuracy= 0.600\n","Step 1360, Minibatch log Loss= 9.273602, Training Accuracy= 0.300\n","Step 1370, Minibatch log Loss= 2.100904, Training Accuracy= 0.700\n","Step 1380, Minibatch log Loss= 4.960101, Training Accuracy= 0.700\n","Step 1390, Minibatch log Loss= 9.459620, Training Accuracy= 0.200\n","Step 1400, Minibatch log Loss= 3.620946, Training Accuracy= 0.600\n","Step 1410, Minibatch log Loss= 6.509684, Training Accuracy= 0.600\n","Step 1420, Minibatch log Loss= 2.450412, Training Accuracy= 0.600\n","Step 1430, Minibatch log Loss= 2.812431, Training Accuracy= 0.800\n","Step 1440, Minibatch log Loss= 2.938038, Training Accuracy= 0.400\n","Step 1450, Minibatch log Loss= 3.877459, Training Accuracy= 0.500\n","Step 1460, Minibatch log Loss= 0.734555, Training Accuracy= 0.800\n","Step 1470, Minibatch log Loss= 0.544759, Training Accuracy= 0.700\n","Step 1480, Minibatch log Loss= 6.447238, Training Accuracy= 0.600\n","Step 1490, Minibatch log Loss= 4.835389, Training Accuracy= 0.700\n","Step 1500, Minibatch log Loss= 1.875988, Training Accuracy= 0.700\n","Step 1510, Minibatch log Loss= 0.289676, Training Accuracy= 0.800\n","Step 1520, Minibatch log Loss= 1.879149, Training Accuracy= 0.800\n","Step 1530, Minibatch log Loss= 3.226008, Training Accuracy= 0.700\n","Step 1540, Minibatch log Loss= 0.438974, Training Accuracy= 0.800\n","Step 1550, Minibatch log Loss= 1.451910, Training Accuracy= 0.300\n","Step 1560, Minibatch log Loss= 4.237948, Training Accuracy= 0.700\n","Step 1570, Minibatch log Loss= 0.353210, Training Accuracy= 0.800\n","Step 1580, Minibatch log Loss= 3.449369, Training Accuracy= 0.700\n","Step 1590, Minibatch log Loss= 0.314894, Training Accuracy= 0.900\n","Step 1600, Minibatch log Loss= 2.153304, Training Accuracy= 0.500\n","Step 1610, Minibatch log Loss= 3.998638, Training Accuracy= 0.600\n","Step 1620, Minibatch log Loss= 0.917076, Training Accuracy= 0.800\n","Step 1630, Minibatch log Loss= 4.623523, Training Accuracy= 0.500\n","Step 1640, Minibatch log Loss= 4.108005, Training Accuracy= 0.300\n","Step 1650, Minibatch log Loss= 1.830909, Training Accuracy= 0.800\n","Step 1660, Minibatch log Loss= 1.872812, Training Accuracy= 0.800\n","Step 1670, Minibatch log Loss= 0.266213, Training Accuracy= 0.800\n","Step 1680, Minibatch log Loss= 0.192085, Training Accuracy= 0.900\n","\n","\n","epoch : 3\n","Step 1690, Minibatch log Loss= 0.562484, Training Accuracy= 0.800\n","Step 1700, Minibatch log Loss= 2.469146, Training Accuracy= 0.800\n","Step 1710, Minibatch log Loss= 0.626711, Training Accuracy= 0.600\n","Step 1720, Minibatch log Loss= 0.335277, Training Accuracy= 0.900\n","Step 1730, Minibatch log Loss= 1.425238, Training Accuracy= 0.700\n","Step 1740, Minibatch log Loss= 4.991384, Training Accuracy= 0.600\n","Step 1750, Minibatch log Loss= 2.219394, Training Accuracy= 0.600\n","Step 1760, Minibatch log Loss= 2.236315, Training Accuracy= 0.800\n","Step 1770, Minibatch log Loss= 3.383918, Training Accuracy= 0.200\n","Step 1780, Minibatch log Loss= 2.002078, Training Accuracy= 0.600\n","Step 1790, Minibatch log Loss= 0.741040, Training Accuracy= 0.400\n","Step 1800, Minibatch log Loss= 3.526046, Training Accuracy= 0.500\n","Step 1810, Minibatch log Loss= 0.540288, Training Accuracy= 0.700\n","Step 1820, Minibatch log Loss= 1.915380, Training Accuracy= 0.800\n","Step 1830, Minibatch log Loss= 3.664165, Training Accuracy= 0.500\n","Step 1840, Minibatch log Loss= 0.498149, Training Accuracy= 0.700\n","Step 1850, Minibatch log Loss= 1.854877, Training Accuracy= 0.800\n","Step 1860, Minibatch log Loss= 1.795589, Training Accuracy= 0.900\n","Step 1870, Minibatch log Loss= 3.512453, Training Accuracy= 0.700\n","Step 1880, Minibatch log Loss= 1.500247, Training Accuracy= 0.700\n","Step 1890, Minibatch log Loss= 0.865924, Training Accuracy= 0.800\n","Step 1900, Minibatch log Loss= 2.649235, Training Accuracy= 0.300\n","Step 1910, Minibatch log Loss= 0.456341, Training Accuracy= 0.700\n","Step 1920, Minibatch log Loss= 1.920197, Training Accuracy= 0.800\n","Step 1930, Minibatch log Loss= 0.460751, Training Accuracy= 0.800\n","Step 1940, Minibatch log Loss= 1.779531, Training Accuracy= 0.600\n","Step 1950, Minibatch log Loss= 1.666799, Training Accuracy= 0.700\n","Step 1960, Minibatch log Loss= 1.609130, Training Accuracy= 0.500\n","Step 1970, Minibatch log Loss= 0.310582, Training Accuracy= 0.900\n","Step 1980, Minibatch log Loss= 0.572987, Training Accuracy= 0.700\n","Step 1990, Minibatch log Loss= 0.311341, Training Accuracy= 0.900\n","Step 2000, Minibatch log Loss= 2.093176, Training Accuracy= 0.600\n","Step 2010, Minibatch log Loss= 2.434685, Training Accuracy= 0.700\n","Step 2020, Minibatch log Loss= 2.212965, Training Accuracy= 0.500\n","Step 2030, Minibatch log Loss= 1.926883, Training Accuracy= 0.800\n","Step 2040, Minibatch log Loss= 0.599493, Training Accuracy= 0.600\n","Step 2050, Minibatch log Loss= 0.674339, Training Accuracy= 0.500\n","Step 2060, Minibatch log Loss= 2.101636, Training Accuracy= 0.600\n","Step 2070, Minibatch log Loss= 0.859436, Training Accuracy= 0.800\n","Step 2080, Minibatch log Loss= 0.489538, Training Accuracy= 0.700\n","Step 2090, Minibatch log Loss= 0.637782, Training Accuracy= 0.600\n","Step 2100, Minibatch log Loss= 0.552072, Training Accuracy= 0.600\n","Step 2110, Minibatch log Loss= 0.488159, Training Accuracy= 0.700\n","Step 2120, Minibatch log Loss= 0.705861, Training Accuracy= 0.800\n","Step 2130, Minibatch log Loss= 0.445125, Training Accuracy= 0.700\n","Step 2140, Minibatch log Loss= 0.276758, Training Accuracy= 0.900\n","Step 2150, Minibatch log Loss= 2.434903, Training Accuracy= 0.200\n","Step 2160, Minibatch log Loss= 0.530361, Training Accuracy= 0.700\n","Step 2170, Minibatch log Loss= 1.732345, Training Accuracy= 0.500\n","Step 2180, Minibatch log Loss= 2.202200, Training Accuracy= 0.500\n","Step 2190, Minibatch log Loss= 1.180225, Training Accuracy= 0.500\n","Step 2200, Minibatch log Loss= 0.690589, Training Accuracy= 0.500\n","Step 2210, Minibatch log Loss= 0.793154, Training Accuracy= 0.400\n","Step 2220, Minibatch log Loss= 0.574480, Training Accuracy= 0.700\n","Step 2230, Minibatch log Loss= 1.949583, Training Accuracy= 0.700\n","Step 2240, Minibatch log Loss= 0.595547, Training Accuracy= 0.500\n","\n","\n","epoch : 4\n","Step 2250, Minibatch log Loss= 0.630004, Training Accuracy= 0.600\n","Step 2260, Minibatch log Loss= 0.576919, Training Accuracy= 0.600\n","Step 2270, Minibatch log Loss= 0.728874, Training Accuracy= 0.500\n","Step 2280, Minibatch log Loss= 0.383755, Training Accuracy= 0.800\n","Step 2290, Minibatch log Loss= 0.575823, Training Accuracy= 0.700\n","Step 2300, Minibatch log Loss= 0.384003, Training Accuracy= 0.800\n","Step 2310, Minibatch log Loss= 0.431213, Training Accuracy= 0.800\n","Step 2320, Minibatch log Loss= 0.726178, Training Accuracy= 0.400\n","Step 2330, Minibatch log Loss= 0.579199, Training Accuracy= 0.600\n","Step 2340, Minibatch log Loss= 0.724247, Training Accuracy= 0.400\n","Step 2350, Minibatch log Loss= 0.774313, Training Accuracy= 0.600\n","Step 2360, Minibatch log Loss= 0.475741, Training Accuracy= 0.800\n","Step 2370, Minibatch log Loss= 0.482397, Training Accuracy= 0.800\n","Step 2380, Minibatch log Loss= 0.576313, Training Accuracy= 0.600\n","Step 2390, Minibatch log Loss= 0.383204, Training Accuracy= 0.700\n","Step 2400, Minibatch log Loss= 0.578419, Training Accuracy= 0.700\n","Step 2410, Minibatch log Loss= 1.096626, Training Accuracy= 0.400\n","Step 2420, Minibatch log Loss= 0.522570, Training Accuracy= 0.500\n","Step 2430, Minibatch log Loss= 0.620968, Training Accuracy= 0.500\n","Step 2440, Minibatch log Loss= 0.669453, Training Accuracy= 0.500\n","Step 2450, Minibatch log Loss= 2.096486, Training Accuracy= 0.700\n","Step 2460, Minibatch log Loss= 0.618221, Training Accuracy= 0.500\n","Step 2470, Minibatch log Loss= 2.321545, Training Accuracy= 0.300\n","Step 2480, Minibatch log Loss= 0.915699, Training Accuracy= 0.500\n","Step 2490, Minibatch log Loss= 0.630757, Training Accuracy= 0.700\n","Step 2500, Minibatch log Loss= 0.401490, Training Accuracy= 0.800\n","Step 2510, Minibatch log Loss= 0.530107, Training Accuracy= 0.700\n","Step 2520, Minibatch log Loss= 0.613293, Training Accuracy= 0.500\n","Step 2530, Minibatch log Loss= 0.433085, Training Accuracy= 0.700\n","Step 2540, Minibatch log Loss= 0.519198, Training Accuracy= 0.600\n","Step 2550, Minibatch log Loss= 0.518757, Training Accuracy= 0.600\n","Step 2560, Minibatch log Loss= 0.336085, Training Accuracy= 0.700\n","Step 2570, Minibatch log Loss= 0.762588, Training Accuracy= 0.600\n","Step 2580, Minibatch log Loss= 0.715997, Training Accuracy= 0.600\n","Step 2590, Minibatch log Loss= 0.752586, Training Accuracy= 0.400\n","Step 2600, Minibatch log Loss= 0.583166, Training Accuracy= 0.700\n","Step 2610, Minibatch log Loss= 0.605655, Training Accuracy= 0.500\n","Step 2620, Minibatch log Loss= 0.551870, Training Accuracy= 0.500\n","Step 2630, Minibatch log Loss= 0.673269, Training Accuracy= 0.600\n","Step 2640, Minibatch log Loss= 1.966309, Training Accuracy= 0.800\n","Step 2650, Minibatch log Loss= 2.071875, Training Accuracy= 0.500\n","Step 2660, Minibatch log Loss= 0.513122, Training Accuracy= 0.600\n","Step 2670, Minibatch log Loss= 0.690232, Training Accuracy= 0.400\n","Step 2680, Minibatch log Loss= 0.566792, Training Accuracy= 0.600\n","Step 2690, Minibatch log Loss= 0.531674, Training Accuracy= 0.700\n","Step 2700, Minibatch log Loss= 0.673385, Training Accuracy= 0.600\n","Step 2710, Minibatch log Loss= 2.143485, Training Accuracy= 0.600\n","Step 2720, Minibatch log Loss= 2.143457, Training Accuracy= 0.600\n","Step 2730, Minibatch log Loss= 0.518551, Training Accuracy= 0.600\n","Step 2740, Minibatch log Loss= 0.742830, Training Accuracy= 0.400\n","Step 2750, Minibatch log Loss= 0.619824, Training Accuracy= 0.600\n","Step 2760, Minibatch log Loss= 0.410890, Training Accuracy= 0.900\n","Step 2770, Minibatch log Loss= 0.685435, Training Accuracy= 0.400\n","Step 2780, Minibatch log Loss= 0.532519, Training Accuracy= 0.700\n","Step 2790, Minibatch log Loss= 0.391401, Training Accuracy= 0.800\n","Step 2800, Minibatch log Loss= 0.391533, Training Accuracy= 0.800\n","Step 2810, Minibatch log Loss= 0.569453, Training Accuracy= 0.500\n","\n","\n","epoch : 5\n","Step 2820, Minibatch log Loss= 0.423463, Training Accuracy= 0.700\n","Step 2830, Minibatch log Loss= 0.619221, Training Accuracy= 0.600\n","Step 2840, Minibatch log Loss= 0.650014, Training Accuracy= 0.500\n","Step 2850, Minibatch log Loss= 0.569197, Training Accuracy= 0.400\n","Step 2860, Minibatch log Loss= 1.562138, Training Accuracy= 0.700\n","Step 2870, Minibatch log Loss= 0.592187, Training Accuracy= 0.500\n","Step 2880, Minibatch log Loss= 0.591252, Training Accuracy= 0.500\n","Step 2890, Minibatch log Loss= 0.800071, Training Accuracy= 0.600\n","Step 2900, Minibatch log Loss= 0.833804, Training Accuracy= 0.400\n","Step 2910, Minibatch log Loss= 0.702464, Training Accuracy= 0.500\n","Step 2920, Minibatch log Loss= 0.561687, Training Accuracy= 0.600\n","Step 2930, Minibatch log Loss= 1.283456, Training Accuracy= 0.400\n","Step 2940, Minibatch log Loss= 0.534869, Training Accuracy= 0.700\n","Step 2950, Minibatch log Loss= 0.728399, Training Accuracy= 0.400\n","Step 2960, Minibatch log Loss= 0.623014, Training Accuracy= 0.800\n","Step 2970, Minibatch log Loss= 0.514250, Training Accuracy= 1.000\n","Step 2980, Minibatch log Loss= 0.675526, Training Accuracy= 0.600\n","Step 2990, Minibatch log Loss= 0.535082, Training Accuracy= 0.700\n","Step 3000, Minibatch log Loss= 0.554928, Training Accuracy= 0.600\n","Step 3010, Minibatch log Loss= 0.702176, Training Accuracy= 0.500\n","Step 3020, Minibatch log Loss= 2.173497, Training Accuracy= 0.500\n","Step 3030, Minibatch log Loss= 0.623575, Training Accuracy= 0.800\n","Step 3040, Minibatch log Loss= 0.535350, Training Accuracy= 0.700\n","Step 3050, Minibatch log Loss= 0.395093, Training Accuracy= 0.800\n","Step 3060, Minibatch log Loss= 0.649726, Training Accuracy= 0.700\n","Step 3070, Minibatch log Loss= 0.561222, Training Accuracy= 0.600\n","Step 3080, Minibatch log Loss= 0.561422, Training Accuracy= 0.600\n","Step 3090, Minibatch log Loss= 0.394964, Training Accuracy= 0.800\n","Step 3100, Minibatch log Loss= 2.007033, Training Accuracy= 0.700\n","Step 3110, Minibatch log Loss= 0.535619, Training Accuracy= 0.700\n","Step 3120, Minibatch log Loss= 0.675860, Training Accuracy= 0.600\n","Step 3130, Minibatch log Loss= 0.618337, Training Accuracy= 0.600\n","Step 3140, Minibatch log Loss= 0.495988, Training Accuracy= 0.700\n","Step 3150, Minibatch log Loss= 0.667306, Training Accuracy= 0.400\n","Step 3160, Minibatch log Loss= 2.205733, Training Accuracy= 0.600\n","Step 3170, Minibatch log Loss= 0.584612, Training Accuracy= 0.500\n","Step 3180, Minibatch log Loss= 0.550032, Training Accuracy= 0.600\n","Step 3190, Minibatch log Loss= 0.641257, Training Accuracy= 0.500\n","Step 3200, Minibatch log Loss= 0.559508, Training Accuracy= 0.600\n","Step 3210, Minibatch log Loss= 0.721317, Training Accuracy= 0.400\n","Step 3220, Minibatch log Loss= 0.698879, Training Accuracy= 0.500\n","Step 3230, Minibatch log Loss= 0.972525, Training Accuracy= 0.400\n","Step 3240, Minibatch log Loss= 0.677496, Training Accuracy= 0.600\n","Step 3250, Minibatch log Loss= 0.661373, Training Accuracy= 0.400\n","Step 3260, Minibatch log Loss= 0.596707, Training Accuracy= 0.700\n","Step 3270, Minibatch log Loss= 0.699022, Training Accuracy= 0.500\n","Step 3280, Minibatch log Loss= 0.596406, Training Accuracy= 0.700\n","Step 3290, Minibatch log Loss= 0.618269, Training Accuracy= 0.600\n","Step 3300, Minibatch log Loss= 0.580707, Training Accuracy= 0.500\n","Step 3310, Minibatch log Loss= 0.681680, Training Accuracy= 0.300\n","Step 3320, Minibatch log Loss= 0.718659, Training Accuracy= 0.400\n","Step 3330, Minibatch log Loss= 0.618335, Training Accuracy= 0.600\n","Step 3340, Minibatch log Loss= 1.869845, Training Accuracy= 0.600\n","Step 3350, Minibatch log Loss= 0.558335, Training Accuracy= 0.600\n","Step 3360, Minibatch log Loss= 0.558304, Training Accuracy= 0.600\n","Step 3370, Minibatch log Loss= 0.597499, Training Accuracy= 0.400\n","\n","\n","optimization complete!\n","Model saved in path: saved_model/model.ckpt\n","\t test accuracy : 0.616375744342804\n"],"name":"stdout"}]},{"metadata":{"id":"NeM9RRhbMJHK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}